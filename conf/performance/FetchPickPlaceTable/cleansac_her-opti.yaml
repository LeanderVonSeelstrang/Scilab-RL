# @package _global_
# Changes specified in this config should be interpreted as relative to the _global_ package.
defaults:
  - override /algorithm: cleansac

n_epochs: 50
eval_after_n_steps: 4000
early_stop_threshold: 0.8
early_stop_data_column: 'eval/success_rate'
hyperopt_criterion: 'train/rollout_rewards_mean'

env: 'FetchPlaceOnTable-v2'

render: 'none' # 'display', 'record', or anything else for neither one
render_freq: 5
render_metrics_train: []
render_metrics_test: []
#render_metrics_train: ['train/rollout_rewards_step',  'mc/i_reward', 'mc/kld', 'actor_entropy']
#render_metrics_test: ['eval/rollout_rewards_step', 'mc/i_reward', 'mc/kld', 'actor_entropy']
render_frames_per_clip: 200

algorithm:
  name: 'cleansac'
  learning_rate: 0.0003
  buffer_size: 1_000_000
  learning_starts: 1000
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  ent_coef: auto
  use_her: True
  n_critics: 2

  # Whether to set future expected discounted cumulative reward to what the critic
  # computes or to just set it to 0 if the episode is done. The SB3 implementation sets it to 0 if done,
  # which is equivalent to false, so we set this as default here.
  ignore_dones_for_qvalue: False

  # Usually, the action scale is determined by the action space of the environment. Sometimes, however, it is
  # desireable to not max out the full scale and reduce the action intensity. For this purpose,
  # action_scale_factor is multiplied with the action space of the environment.
  action_scale_factor: 1.0

  # Whether to log each observation and action dimension in each step. This might slow down everything, but it can help
  # debugging because it logs each action and observation dimension per step. It only holds for training, not for eval.
  log_obs_step: False
  log_act_step: False

hydra:
  sweeper:
    study_name: cleansacher_FetchPickPlaceTable
    max_trials: 192
    n_jobs: 48
    direction: maximize
    max_duration_minutes: 18000
    min_trials_per_param: 3
    max_trials_per_param: 6
    search_space:
      ++algorithm.learning_rate:
        type: float
        low: 0.0001
        high: 0.005
        log: true
      ++algorithm.n_critics:
        type: int
        low: 1
        high: 3
      ++algorithm.batch_size:
        type: int
        low: 256
        high: 1024
        step: 256
      ++algorithm.action_scale_factor:
        type: float
        low: 0.4
        high: 1.2
        step: 0.2
      ++algorithm.tau:
        type: float
        low: 0.002
        high: 0.1
        step: 0.002
