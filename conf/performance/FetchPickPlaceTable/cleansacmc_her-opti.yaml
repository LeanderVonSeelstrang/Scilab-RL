
# @package _global_
# Changes specified in this config should be interpreted as relative to the _global_ package.
defaults:
  - override /algorithm: cleansacmc

n_epochs: 100
eval_after_n_steps: 2000
early_stop_threshold: 0.6
early_stop_data_column: 'eval/success_rate'
hyperopt_criterion: 'train/rollout_rewards_mean'

env: 'FetchPlaceOnTable-v2'

render: 'none' # 'display', 'record', or anything else for neither one
render_freq: 5
#render_metrics_train: ['train/rollout_rewards_step',  'mc/i_reward', 'mc/kld', 'actor_entropy']
#render_metrics_test: ['eval/rollout_rewards_step', 'mc/i_reward', 'mc/kld', 'actor_entropy']
render_metrics_train: []
render_metrics_test: []
render_frames_per_clip: 200

algorithm:
  set_min_norm: -1
  set_max_norm: 0
  name: 'cleansacmc'
  learning_rate: 0.0003
  ent_coef: auto
  buffer_size: 1_000_000
  learning_starts: 100
  batch_size: 768
  gamma: 0.99
  tau: 0.01
  use_her: True
  n_critics: 2
  action_scale_factor: 1.0

hydra:
  sweeper:
    study_name: cleansacmc_FetchPickPlaceTable
    max_trials: 128
    n_jobs: 48
    direction: maximize
    max_duration_minutes: 18000
    min_trials_per_param: 3
    max_trials_per_param: 9
    search_space:
      # SAC
      ++algorithm.mc_alpha:
        type: float
        low: 0.0
        high: 0.8
        step: 0.2

